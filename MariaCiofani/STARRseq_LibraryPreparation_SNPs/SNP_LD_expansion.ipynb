{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANCESTRY based SNP identification and LD expansion for each disorder\n",
    "\n",
    "SNPS associated with all 4 disorders (MS, IBD, RA, PS) were wrangled from EBI GWAS catalog and then were separated into 4 lists based on the ancestries (Ancestries under consideration - EUR for European, ASW for African, EAS for East Asian and SAS for South Asian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNP data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gwas_snps_all = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/gwas_snps_ancestry_disorder.txt', sep='\\t')\n",
    "\n",
    "# european ancestry snp list\n",
    "# gwas_snps_european = gwas_snps_all[(gwas_snps_all.ANCESTRY=='EUR') | (gwas_snps_all.ANCESTRY=='EUR ')]\n",
    "# european_snp_list = gwas_snps_european.SNPS.unique().tolist()\n",
    "# with open('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/european_snp_list.txt', \"w\") as f:\n",
    "#     for snp in european_snp_list:\n",
    "#         f.write(\"%s\\n\" % snp)\n",
    "\n",
    "## east asian ancestry snp_list\n",
    "gwas_snps_eastasian = gwas_snps_all[gwas_snps_all.ANCESTRY=='EAS']\n",
    "eastasian_snp_list = gwas_snps_eastasian.SNPS.unique().tolist()\n",
    "with open('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/eastasian_snp_list.txt', \"w\") as f:\n",
    "    for snp in eastasian_snp_list:\n",
    "        f.write(\"%s\\n\" % snp)\n",
    "\n",
    "## african ancestry snp_list\n",
    "gwas_snps_african = gwas_snps_all[gwas_snps_all.ANCESTRY=='ASW']\n",
    "african_snp_list = gwas_snps_african.SNPS.unique().tolist()\n",
    "with open('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/african_snp_list.txt', \"w\") as f:\n",
    "    for snp in african_snp_list:\n",
    "        f.write(\"%s\\n\" % snp)\n",
    "\n",
    "## south asian ancestry snp_list\n",
    "gwas_snps_southasian = gwas_snps_all[gwas_snps_all.ANCESTRY=='SAS']\n",
    "southasian_snp_list = gwas_snps_southasian.SNPS.unique().tolist()\n",
    "with open('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/southasian_snp_list.txt', \"w\") as f:\n",
    "    for snp in southasian_snp_list:\n",
    "        f.write(\"%s\\n\" % snp)\n",
    "\n",
    "print(len(southasian_snp_list))\n",
    "print(len(african_snp_list))\n",
    "print(len(eastasian_snp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gwas_snps_all = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/gwas_snps_ancestry_disorder.txt', sep='\\t')\n",
    "gwas_snps_all = gwas_snps_all.drop_duplicates()\n",
    "gwas_snps_all_list = gwas_snps_all.SNPS.unique().tolist()\n",
    "\n",
    "# #gwas_snps_all['ANCESTRY'] = gwas_snps_all.groupby(['SNPS'])['ANCESTRY'].transform(lambda x: ','.join(x))\n",
    "# gwas_snps_all['CATEGORY'] = gwas_snps_all.groupby(['SNPS', 'ANCESTRY'])['CATEGORY'].transform(lambda x: ','.join(x))\n",
    "# gwas_snps_all_dedup = gwas_snps_all[['SNPS','ANCESTRY','CATEGORY']].drop_duplicates()\n",
    "#gwas_snps_all.to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/gwas_snps_ancestry_disorder_deduplicated.txt', sep='\\t', index=False)\n",
    "#gwas_snps_all[gwas_snps_all['SNPS']=='rs112768831']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_futures.sessions import FuturesSession\n",
    "from concurrent.futures import as_completed\n",
    "import time\n",
    "import json\n",
    "\n",
    "def getVarID(chrom, start):\n",
    "    chrom = int(chrom)\n",
    "    start = int(start)\n",
    "    end = start + 1\n",
    "    headers={ \"Content-Type\" : \"application/json\", \"Accept\" : \"application/json\"}\n",
    "    r = requests.post(\"https://rest.ensembl.org/ga4gh/variants/search\", headers = headers,\n",
    "                     data = '{ \"variantSetId\" : 1, \"referenceName\" : %d, \"start\" : %d, \"end\" : %d }' % (chrom, start,end))\n",
    "    try:\n",
    "        result = r.json()['variants'][0]['names'][0]\n",
    "    except:\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([x for x in gwas_snps_all_list if x.startswith('chr')]).str.split(':', expand = True)\n",
    "s[2] = s[1].astype(int) + 1\n",
    "s[1] = s[1].str.replace(' ', '')\n",
    "s[3] = s[0].astype(str)+':'+s[1].astype(str)\n",
    "s.to_csv(\"/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id_hg19.bed\", sep=\"\\t\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download liftOver chain files to convert to hg38\n",
    "!curl 'http://hgdownload.soe.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz' | gunzip > \"/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/hg19ToHg38.over.chain\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/data/reddylab/Revathy/software/liftOver/liftOver \\\n",
    "/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id_hg19.bed \\\n",
    "/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/hg19ToHg38.over.chain \\\n",
    "/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id_hg38.bed \\\n",
    "/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id.unmapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id_hg38.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifted = pd.read_table(\"/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/no_id_hg38.bed\", header=None)\n",
    "lifted[0] = lifted[0].str.strip(\"chr\")\n",
    "lifted['id'] = ''\n",
    "\n",
    "for ix, row in lifted.iterrows():\n",
    "    lifted.loc[ix, 'id'] = getVarID(row[0], row[1])\n",
    "lifted.to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/lifted_ids.txt', sep='\\t', index=False)\n",
    "\n",
    "lifted_ids = [x for x in lifted['id'] if str(x).startswith('rs')]\n",
    "print(len(lifted_ids), \"rsIDs recovered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifted = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/lifted_ids.txt', sep='\\t')\n",
    "lifted = lifted.rename(columns={'3':'SNPS'})\n",
    "lifted = lifted[['SNPS','id']]\n",
    "lifted = lifted.dropna()\n",
    "lifted_ids = lifted.id.tolist()\n",
    "lifted_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_ids = gwas_snps_all[gwas_snps_all['SNPS'].str.startswith('chr')]\n",
    "chr_ids['SNPS'] = chr_ids['SNPS'].str.replace(' ', '')\n",
    "chrID_rsID = chr_ids.merge(lifted, on='SNPS', how='inner').drop_duplicates()\n",
    "chrID_rsID = chrID_rsID[['id','ANCESTRY','CATEGORY']]\n",
    "chrID_rsID = chrID_rsID.rename(columns={'id':'SNPS'})\n",
    "\n",
    "### remove from gwas_snps_all SNPS which doesnt start with rsID and concatenate the mapped ones\n",
    "\n",
    "gwas_rsID = gwas_snps_all[gwas_snps_all['SNPS'].str.startswith('rs')]\n",
    "gwas_snps_rsID_mapped = pd.concat([gwas_rsID,chrID_rsID])\n",
    "gwas_snps_rsID_mapped['origin'] = 'gwas'\n",
    "gwas_snps_rsID_mapped.to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/gwas_snps_all_rsID_mapped.txt', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_snps_rsID_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "european_snp = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/european_snp_list.txt', sep='\\t', names=['snp_id'])\n",
    "european_snp = european_snp[european_snp['snp_id'].str.startswith('rs')]\n",
    "\n",
    "snp_euro_list = list(european_snp['snp_id'])\n",
    "\n",
    "# problem_list = ['DRB*15:01','chr2:2062560332','DRB*08:01','DRB*03:01','DRB*13:03','A*02:01','chr1:208286009','chr10:2081043743','chr16:2011354091','chr16:2031021078','chr2:2061072183','chr18:2051816394','chr22:2021974703','chr11:20128391937','chr1:20152591953','chr6:20111929862','chr6:2030916259','chr3:20189662658','chr14:2035839236','chr11:20109962432','chr2:20163167746','chr5:20150469973','chr6:2020689945',\n",
    "#                 'chr1:2067713346','chr19:2010463118','chr17:2026124908','chr13:2040745693','chr5:20158829527','chr6:20138197824','chr1:2024518206','chr5:2096118852','chr12:2056741228','chr5:20131996445','chr20:2048574454','chr3:2016996623','chr21:2036488822','chr10:2075601596','chr1:2025291010','chr19:2010886206','chr13:2045321731','chr1:20197757846','chr6:20159506600','chr6:20577820','chr17:2078175483','chr9:2032523737',\n",
    "#                 'chr17:2040536396','chr11:2064053157','chr7:2037385365','chr3:20101647309','chr9:20110792282','chrX:78464616','chr17:38031857','2-62564875','chr1:2523811','chr6:14103212','chr11:107967350','chr21:35928240','chr7:128580042','chr8:129011095','chr20:52210360','chr6:119215402','chr10:6390285','rs67934705','chr5:1519833','chr13:100026952','chr3:5035903','chr6:14691215','chr3:121783015',\n",
    "#                 'chr3:100656795','chr3:121765368','chr3:100848597','chr1:198573373','chr14:88523488','chr3:112693983','chr1:1682374','chr1:154983036','chr16:68694818','chr1:150593391','chr11:118783424','chr11:14868316','chr16:11213951','chr1:32715071','chr1:32738415','chr16:11353879','chr16:68335911','chr18:60902282','chr2:112492986','chr5:40429250','chr8:129177769','chr20:52783991','chr8:95851818',\n",
    "#                 'chr7:50328339','chr9:86543849','rs34723276','chr3:47005668','chr6:130348257','chr16:30171625','chr10:21867179','chr1:208286011','chr10:2081043743','chr16:2031021078','chr16:2011354091','chr2:2062560332','chr18:2051816394','chr22:2021974703','chr11:20109962432','chr1:20152591953','chr3:20189662658','chr14:2035839236','chr6:2020689945','chr2:2061072183','chr1:2067713346','chr17:2026124908','chr11:20128391937','chr2:20163167746',\n",
    "#                 'chr6:20111929862','chr5:20150469973','chr6:2030916259','chr12:2056741228','chr19:2010463118','chr13:2040745693','chr5:20158829527','chr6:20138197824','chr5:20131996445','chr20:2048574454','chr10:2075601596','chr17:2040536396','chr21:2036488822','chr1:2025291010','chr3:20101647309','chr5:2096118852','chr1:2024518206','chr6:20577820','chr7:2037385365','chr3:2016996623','chr19:2010886206','chr1:20197757846','chr6:20159506600',\n",
    "#                 'chr9:2032523737','chr13:2045321731','chr11:2064053157','chr17:2078175483','chr9:20110792282']\n",
    "\n",
    "\n",
    "# problem_df = pd.DataFrame(problem_list, columns=['snp_id'])\n",
    "\n",
    "# final_european_snp = pd.concat([european_snp,problem_df], axis=0, ignore_index=True).drop_duplicates(keep=False)\n",
    "# final_european_snp_list = list(final_european_snp['snp_id'])\n",
    "\n",
    "\n",
    "european_snp_list = snp_euro_list + lifted_ids\n",
    "len(european_snp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LD expansion based on ancestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from requests_futures.sessions import FuturesSession\n",
    "from concurrent.futures import as_completed\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "def ensembl_LD(snp_list):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check input\n",
    "    if not isinstance(snp_list, list):\n",
    "        raise TypeError(\"The input is not a list!\")\n",
    "        \n",
    "    # Init empty dict for storing results and list for problem URLs\n",
    "    ld_snps = {\"query\" : [],\n",
    "             \"result\" : [],\n",
    "             \"r2\" : [],\n",
    "             \"d_prime\" : []}\n",
    "    problems = []\n",
    "    \n",
    "    # Start up the non-blocking requests\n",
    "    session = FuturesSession(max_workers=10)\n",
    "    futures = [session.get(\"https://rest.ensembl.org/ld/human/{}/1000GENOMES:phase_3:EUR?\".format(snp),\n",
    "                          headers={ \"Content-Type\" : \"application/json\"}) for snp in snp_list]\n",
    "    \n",
    "    # As the requests complete, try to parse the data into the results, otherwise report a problem URL\n",
    "    for f in as_completed(futures):\n",
    "        try:\n",
    "            json_data = f.result().json()\n",
    "            for rec in range(0, len(json_data)):\n",
    "                ld_snps['query'].append(json_data[rec]['variation1'])\n",
    "                ld_snps['result'].append(json_data[rec]['variation2'])\n",
    "                ld_snps['r2'].append(json_data[rec]['r2'])\n",
    "                ld_snps['d_prime'].append(json_data[rec]['d_prime'])\n",
    "        except:\n",
    "            problems.append(f.result().request.url)\n",
    "            print(\"Found a problem. Will be requeued.\")\n",
    "            \n",
    "    # Requeue the problem URLs in case there was a fluke\n",
    "    session = FuturesSession(max_workers=8)\n",
    "    futures = [session.get(url,\n",
    "                          headers={ \"Content-Type\" : \"application/json\"}) for url in problems]\n",
    "    problems2 = []\n",
    "    for f in as_completed(futures):\n",
    "        try:\n",
    "            json_data = f.result().json()\n",
    "            for rec in range(0, len(json_data)):\n",
    "                ld_snps['query'].append(json_data[rec]['variation1'])\n",
    "                ld_snps['result'].append(json_data[rec]['variation2'])\n",
    "                ld_snps['r2'].append(json_data[rec]['r2'])\n",
    "                ld_snps['d_prime'].append(json_data[rec]['d_prime'])\n",
    "            print(\"Succeeded on requeue!\")\n",
    "        except:\n",
    "            problems2.append(f.result().request.url)\n",
    "            print(\"Failed on a requeued problem.\")\n",
    "            \n",
    "    print(\"Done!\")\n",
    "    print(\"Elapsed: {0} minutes {1:0f} seconds\".format((time.time() - start_time) // 60, (time.time() - start_time) % 60))\n",
    "    ld_snps = pd.DataFrame.from_dict(ld_snps)\n",
    "    return ld_snps, problems2\n",
    "\n",
    "def ensembl_snp_info(snp_list):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check input\n",
    "    if not isinstance(snp_list, list):\n",
    "        raise TypeError(\"The input is not a list!\")\n",
    "        \n",
    "    result_dict = {}\n",
    "    problems = []\n",
    "    \n",
    "    # Start up the non-blocking requests in batches of 100 SNPs per request\n",
    "    session = FuturesSession(max_workers=10)\n",
    "    batch = 0\n",
    "    futures = []\n",
    "    \n",
    "    headers={\"Content-Type\" : \"application/json\",\n",
    "             \"Accept\" : \"application/json\"}\n",
    "    \n",
    "    if len(snp_list) < 100:\n",
    "        futures.append(session.post(\"https://rest.ensembl.org/variation/homo_sapiens\", \n",
    "                                    headers=headers,\n",
    "                                    data = json.dumps({'ids' : snp_list})))\n",
    "\n",
    "    else:\n",
    "        for i in range(0, len(snp_list)//100):\n",
    "            futures.append(session.post(\"https://rest.ensembl.org/variation/homo_sapiens\", \n",
    "                                        headers=headers,\n",
    "                                        data = json.dumps({'ids' : snp_list[batch*100:(batch+1)*100]})))\n",
    "            batch += 1\n",
    "        \n",
    "        # Add the last batch (with less than 100 IDs)\n",
    "        futures.append(session.post(\"https://rest.ensembl.org/variation/homo_sapiens\", \n",
    "                                    headers=headers,\n",
    "                                    data = json.dumps({'ids' : snp_list[(batch)*100:]})))\n",
    "\n",
    "    # As the requests complete, try to parse the data into the results_dict, otherwise report a problem URL\n",
    "    n_complete = 0\n",
    "    n_total = len(snp_list)\n",
    "    for f in as_completed(futures):\n",
    "        try:\n",
    "            json_data = f.result().json()\n",
    "            result_dict.update(json_data)\n",
    "            n_complete += 1\n",
    "            print(\"Finished {} of {}\".format(n_complete, n_total))\n",
    "\n",
    "            \n",
    "        except:\n",
    "            #problems.append(f.result().request.url)\n",
    "            problems = problems + json.loads(f.result().request.body)['ids']\n",
    "            print(\"Found a problem.\")\n",
    "    print(\"Done!\")\n",
    "    print(\"Elapsed: {0} minutes {1:0f} seconds\".format((time.time() - start_time) // 60, (time.time() - start_time) % 60))\n",
    "    #snp_info = pd.DataFrame.from_dict(result_dict)\n",
    "    all_snp_info = {'snp_id' : [],\n",
    "                    'MAF' :[],\n",
    "                    'minor_allele' : [],\n",
    "                    'consequence' : [],\n",
    "                    'allele_string' : [],\n",
    "                    'location' : [],\n",
    "                    'synonyms' : []}\n",
    "    for key in result_dict.keys():\n",
    "        all_snp_info['snp_id'].append(key)\n",
    "        all_snp_info['MAF'].append(result_dict[key]['MAF'])\n",
    "        all_snp_info['minor_allele'].append(result_dict[key]['minor_allele'])\n",
    "        all_snp_info['synonyms'].append(result_dict[key]['synonyms'])\n",
    "        all_snp_info['consequence'].append(result_dict[key]['most_severe_consequence'])\n",
    "    # Some of these have missing mappings? Or maybe it isn't a list?\n",
    "        try:\n",
    "            all_snp_info['allele_string'].append(result_dict[key]['mappings'][0]['allele_string'])\n",
    "            all_snp_info['location'].append(result_dict[key]['mappings'][0]['location'])\n",
    "        except:\n",
    "            print(result_dict[key])\n",
    "            all_snp_info['allele_string'].append(None)\n",
    "            all_snp_info['location'].append(None)\n",
    "    \n",
    "    return all_snp_info\n",
    "\n",
    "ld_snps, problems = ensembl_LD(african_snp_list)\n",
    "ld_snps.r2 = ld_snps.r2.astype(float)\n",
    "ld_snps_threshold = ld_snps[ld_snps.r2 >= 0.8]\n",
    "ld_snps_threshold_list = ld_snps_threshold['result'].unique().tolist()\n",
    "\n",
    "ld_snp_info = ensembl_snp_info(ld_snps_threshold_list)\n",
    "all_snp_df = pd.DataFrame.from_dict(ld_snp_info)\n",
    "\n",
    "ld_snps_with_info = ld_snps_threshold.merge(all_snp_df, left_on='result', right_on='snp_id', how='left')\n",
    "\n",
    "coords = ld_snps_with_info['location'].str.split(r\":|-\", expand=True)\n",
    "ld_snps_with_info['chrom'], ld_snps_with_info['start'], ld_snps_with_info['end'] = 'chr' + coords[0], coords[1], coords[2]\n",
    "ld_snps_with_info.drop(columns=['location'], inplace=True)\n",
    "ld_snps_with_info.to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_african.txt', sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate all the ld_expansions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_eastasian.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "european_df = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_european.txt', sep='\\t')\n",
    "european_df['category'] = 'European'\n",
    "\n",
    "eastasian_df = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_eastasian.txt', sep='\\t')\n",
    "eastasian_df['category'] = 'EastAsian'\n",
    "\n",
    "southasian_df = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_southasian.txt', sep='\\t')\n",
    "southasian_df['category'] = 'SouthAsian'\n",
    "\n",
    "african_df = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_african.txt', sep='\\t')\n",
    "african_df['category'] = 'African'\n",
    "\n",
    "all_snp_df = pd.concat([european_df,eastasian_df,southasian_df,african_df])\n",
    "all_snp_df = all_snp_df.dropna(subset=['chrom'])\n",
    "all_snp_df = all_snp_df.merge(gwas_snps_rsID_mapped, left_on='query', right_on='SNPS', how='inner')\n",
    "all_snp_df = all_snp_df.drop_duplicates()\n",
    "all_snp_df\n",
    "\n",
    "# all_except_eur = all_snp_df[(all_snp_df['category']=='SouthAsian') | (all_snp_df['category']=='EastAsian') | (all_snp_df['category']=='African')]\n",
    "# all_except_eur\n",
    "# all_snp_df.to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snp.txt', sep='\\t', index=False)\n",
    "# #all_snp_df[all_snp_df.isnull().any(axis=1)]\n",
    "# all_snp_df[all_snp_df['query']=='rs12601925']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/ld_snps_european.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(european_df.snp_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_snp_df['result'][all_snp_df['CATEGORY']=='RA'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_snp_df = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snp.txt', sep='\\t')\n",
    "ld_to_bed = all_snp_df[['chrom']]\n",
    "ld_to_bed['start'] = all_snp_df['start'].astype(int)\n",
    "ld_to_bed['end'] = all_snp_df['end'].astype(int)\n",
    "ld_to_bed['name'] = all_snp_df['snp_id']\n",
    "ld_to_bed['ancestry'] = all_snp_df['category']\n",
    "ld_to_bed['disorder'] = all_snp_df['CATEGORY']\n",
    "ld_to_bed['allele_string'] = all_snp_df['allele_string']\n",
    "ld_to_bed['minor_allele'] = all_snp_df['minor_allele']\n",
    "ld_to_bed = ld_to_bed.drop_duplicates()\n",
    "\n",
    "ld_to_bed_annotated = ld_to_bed.merge(gwas_snps_rsID_mapped, left_on='name', right_on='SNPS', how='left')\n",
    "ld_to_bed_annotated = ld_to_bed_annotated.drop(columns = ['SNPS','CATEGORY','ANCESTRY'])\n",
    "ld_to_bed_annotated['origin'] = ld_to_bed_annotated['origin'].fillna('ld')\n",
    "# ld_to_bed_annotated[ld_to_bed_annotated.origin=='ld']\n",
    "\n",
    "ld_to_bed_annotated.to_csv(\"/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snps_r2_0.8.bed\", sep=\"\\t\", header=None, index=None)\n",
    "# len(ld_to_bed_annotated['name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry\n",
    "sort -k1,1 -k2,2n all_ld_snps_r2_0.8.bed > all_ld_snps_r2_0.8.sorted.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_to_bed_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snps_r2_0.8.sorted.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all_ld_snp = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snps_r2_0.8.sorted.bed', sep='\\t',\n",
    "                         names=['chr','start','end','snp_id','ancestry','disorder','allele_string','minor_allele','origin'])\n",
    "all_ld_snp['difference'] = all_ld_snp['end'] - all_ld_snp['start']\n",
    "all_ld_snp['end'] = np.where(all_ld_snp['difference']<0, all_ld_snp['end']+np.abs(all_ld_snp['difference']) , all_ld_snp['end'])\n",
    "\n",
    "all_ld_snp.drop_duplicates().to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_ld_snps_r2_0.8_new.sorted.bed', sep='\\t',index=False, header=None)\n",
    "all_ld_snp\n",
    "\n",
    "# # all_ld_snp_point = all_ld_snp[all_ld_snp['difference']==0]\n",
    "# all_ld_snp_non_point = all_ld_snp[(all_ld_snp['difference']<=12)&(all_ld_snp['difference']!=0)&(all_ld_snp['difference']!=-1)]\n",
    "\n",
    "# # all_ld_snp_point = all_ld_snp_point.drop(columns='difference')\n",
    "# all_ld_snp_non_point = all_ld_snp_non_point.drop(columns='difference')\n",
    "# all_ld_snp_non_point.drop_duplicates().to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_non_point_ld_snps_r2_0.8.sorted.bed', sep='\\t',index=False, header=None)\n",
    "# all_ld_snp_point.drop_duplicates().to_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/ancestry/all_point_ld_snps_r2_0.8.sorted.bed', sep='\\t',index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ld_snp_point = all_ld_snp_point.drop_duplicates()\n",
    "all_ld_snp_point[all_ld_snp_point.origin=='gwas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Query SNPs in MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gwas_snps_all = pd.read_csv('/data/reddylab/Revathy/collabs/Maria/human-th-ms_new/data_v1/snp_data/source_snps/gwas_snps_ancestry_disorder.txt', sep='\\t')\n",
    "gwas_snps_all_list = gwas_snps_all.SNPS.unique().tolist()\n",
    "# gwas_snps_ms = set(gwas_snps_all['SNPS'][gwas_snps_all['CATEGORY']=='MS'].unique())\n",
    "# len(gwas_snps_ms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}